{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from MLP_Implementation import MLP\n",
    "\n",
    "from PyTorch_Vanilla import RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor from a list \n",
    "a = torch.tensor([0, 1, 2])\n",
    "\n",
    "# Tensor from a NumPy array\n",
    "c = np.ones([2, 3])\n",
    "c = torch.tensor(c)\n",
    "\n",
    "# Additional common constructors\n",
    "x = torch.ones(5, 3)\n",
    "y = torch.linspace(0, 5, steps = 11)\n",
    "\n",
    "# We can also compute sums and means of all the elemnts, and also across specific dimensions\n",
    "z = x.mean(dim = (0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1],\n",
      "        [ 2,  3],\n",
      "        [ 4,  5],\n",
      "        [ 6,  7],\n",
      "        [ 8,  9],\n",
      "        [10, 11]])\n",
      "torch.Size([6, 2])\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Reshaping a Tensor\n",
    "x = torch.arange(12)\n",
    "y = x.reshape(-1, 2)\n",
    "\n",
    "print(y)\n",
    "print(y.shape)\n",
    "print(x.data_ptr == y.data_ptr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([48, 64, 3])\n",
      "torch.Size([3, 64, 48])\n"
     ]
    }
   ],
   "source": [
    "# Permuting a Tensor\n",
    "x = torch.rand(3, 48, 64)\n",
    "y = x.permute(1, 2, 0)\n",
    "print(y.shape)\n",
    "\n",
    "# Transposing a Tensor\n",
    "z = x.transpose(2, 1)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n"
     ]
    }
   ],
   "source": [
    "# Flatten a tensor\n",
    "x = torch.arange(12)\n",
    "y = x.reshape(-1, 2)\n",
    "z = y.flatten()\n",
    "\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1])\n",
      "tensor([[0.0034, 0.0049],\n",
      "        [0.2350, 0.5288]])\n"
     ]
    }
   ],
   "source": [
    "# Unsqueezing a Tensor for broadcasting \n",
    "x = torch.rand(2)\n",
    "y = torch.rand(2, 2)\n",
    "# z = x * y # results in an error\n",
    "\n",
    "x = x.unsqueeze(1)\n",
    "print(x.shape)\n",
    "z = x*y\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 8])\n",
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# Concatenation \n",
    "x = torch.rand(3, 4)\n",
    "y = torch.rand(3, 4)\n",
    "z1 = torch.cat((x, y), dim = 1)\n",
    "\n",
    "# Stacking\n",
    "z2 = torch.stack((x, y), dim = 0)\n",
    "print(z1.shape)\n",
    "print(z2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector- vector multiplication\n",
    "x = torch.rand(5)\n",
    "y = torch.rand(5)\n",
    "z = x.dot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 8])\n"
     ]
    }
   ],
   "source": [
    "net = MLP(4, 8, [32, 16])\n",
    "\n",
    "x = torch.randn(20, 4)\n",
    "y = net(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2757224832.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[85], line 6\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(W.requires_gra d)\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "W = torch.randn(8, 12, requires_grad = True)\n",
    "x = torch.randn(12)\n",
    "y = W @ x\n",
    "z = y.detach()\n",
    "\n",
    "print(W.requires_gra d)\n",
    "print(y.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructs an MLP with architecture 4 -> 16 -> 2 for solving a binary classificiation problem\n",
    "net = MLP(4, 2, [16])\n",
    "\n",
    "# Create synthetic data and feed to network \n",
    "inputs = torch.randn(32, 4)\n",
    "labels = torch.randint(2, size = (32,))\n",
    "\n",
    "output = net(inputs)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() # Handles the softmax, which maximizes our log likelihood\n",
    "loss = loss_fn(output, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m net \u001b[38;5;241m=\u001b[39m RNN(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m8\u001b[39m)\n\u001b[1;32m      3\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m y,_ \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(y\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/anaconda3/envs/cse599n/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Downloads/CSE 599N /PyTorch Tutorial/PyTorch_Vanilla.py:14\u001b[0m, in \u001b[0;36mRNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     11\u001b[0m     rnn_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn(x) \u001b[38;5;66;03m# Running the RNN is as easy as passing te input to the object (BatchSize x Sequence Length x Input Dimension)\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     batch_size, T, _ \u001b[38;5;241m=\u001b[39m \u001b[43mrnn_out\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n\u001b[1;32m     15\u001b[0m     rnn_out_reshaped \u001b[38;5;241m=\u001b[39m rnn_out\u001b[38;5;241m.\u001b[39mreshape(batch_size \u001b[38;5;241m*\u001b[39m T, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# We want to combine the batch and sequence dimension, since the decoder doesn't depend on time\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     decoder_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(rnn_out_reshaped) \u001b[38;5;66;03m# Pass the hidden states to the decoder\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "net = RNN(4, 16, 8)\n",
    "\n",
    "x = torch.randn(20, 32, 4)\n",
    "y,_ = net(x)\n",
    "print(y.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse599n",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
