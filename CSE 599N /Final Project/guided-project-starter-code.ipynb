{"cells":[{"cell_type":"markdown","metadata":{"id":"UAJhrBzLoUVy"},"source":["# Guided Final Project\n","\n"]},{"cell_type":"markdown","metadata":{"id":"tqnHokUt9eOL"},"source":["For your final project, you will be implementing a variational autoencoder (VAE) and applying it to neural data.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"URNC2zb_pYFu"},"source":["### Copying this Colab Notebook to your Google Drive"]},{"cell_type":"markdown","metadata":{"id":"eiEeHZNZpaKU"},"source":["Since the course staff is the author of this notebook, you cannot make any lasting changes to it. You should make a copy of it to your Google Drive.\n","\n","Alternatively, you may download a copy and work locally."]},{"cell_type":"markdown","metadata":{"id":"5Uia4S9Op1zG"},"source":["### Compute"]},{"cell_type":"markdown","metadata":{"id":"3Nxz3Yeqp3kG"},"source":["You should not need to use the GPU for this problem. If you do choose to accelerate your training runs with the GPU, keep in mind that Google Colab will limit your GPU usage. You can change your runtime by going to **Runtime -> Change runtime type**.\n","\n","If Google Colab restricts your access to a free GPU, you will regain access to the GPU after an indeterminate amount of time (anecdotally, anywhere from a few hours to a day). In the meantime, you can switch your runtime back to CPU."]},{"cell_type":"markdown","metadata":{"id":"5UQyI2tOqXIL"},"source":["### Code: Setup"]},{"cell_type":"markdown","metadata":{"id":"wIB7nH2kqYvy"},"source":["We start by importing some packages that we'll need to complete this problem and some basic scaffolding."]},{"cell_type":"code","execution_count":45,"metadata":{"id":"a2TIjan_sEGC"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import torch.distributions as dist\n","\n","import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"i3HhPV1Pt5N5"},"outputs":[],"source":["def get_device():\n","    \"\"\"\n","    Helper function to set the device. CUDA if available, else MPS if available (Apple Silicon,\n","    if you are working locally on a newer Apple device), CPU otherwise.\n","\n","    Args:\n","        None.\n","\n","    Returns:\n","        Device string (\"cuda\", \"mps\" or \"cpu\").\n","    \"\"\"\n","    if torch.backends.cuda.is_built() and torch.cuda.is_available():\n","        device = \"cuda\"\n","        print(\"CUDA GPU enabled.\")\n","    elif torch.backends.mps.is_built() and torch.backends.mps.is_available():\n","        device = \"mps\"\n","        print(\"Apple Silicon GPU enabled.\")\n","    else:\n","        device = \"cpu\"\n","        print(\"No GPU found. Running on CPU.\")\n","\n","    return device"]},{"cell_type":"markdown","metadata":{"id":"9ytTJ5ZhrA3w"},"source":["We will be using the neural data from HW2, Question 4, where we consider the delayed reach task that we've discussed in depth now. To load the data, first download the ```data.zip``` file from Canvas (available under the \"Final Project\" module) and place into the \"Files\" tab on the left hand side of this Colab notebook. Then, run the following cell to load in the data.\n","\n","*Note: Once your Colab runtime has disconnected (when you close out of the notebook or change your runtime type), you will have to reload the ```data.zip``` file into your new runtime. To avoid this, you can save the ```data.zip``` file to your Google Drive and mount your Google Drive to your Colab notebook, using the following two lines:*\n","```\n","from google.colab import drive\n","drive.mount('/content/drive')\n","```\n","*If you choose to mount your Google Drive, update the following cells and data paths to make sure you are loading in the correct files.*"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"iG7BWI1gwqha"},"outputs":[],"source":["train_data_loaded = np.load(\"data/ps2a_train.npy\").astype(\"float32\")\n","test_data_loaded = np.load(\"data/ps2a_test.npy\").astype(\"float32\")\n","num_class, num_train_trials, num_features = train_data_loaded.shape\n","_, num_test_trials, _ = test_data_loaded.shape\n","\n","# reshape raw data into the format that we want\n","train_data = train_data_loaded.reshape((-1, num_features))\n","test_data = test_data_loaded.reshape((-1, num_features))\n","# include labels for each class\n","train_labels = np.repeat(np.arange(8), num_train_trials)\n","test_labels = np.repeat(np.arange(8), num_test_trials)\n","\n","num_observations, _ = train_data.shape\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Jvqr2AthkBJ8"},"source":["Now, we define a simple dataset class to format the data to utilize the PyTorch library:"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"N8oqT11kkAlY"},"outputs":[],"source":["class SimpleDataset(Dataset):\n","    def __init__(self, data_bxd, labels, device='cpu'):\n","        super().__init__()\n","        self.device = device\n","        self.data_bxd = data_bxd\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return self.data_bxd.shape[0]\n","\n","    def __getitem__(self, idx):\n","        return torch.tensor(self.data_bxd[idx], device=self.device), torch.tensor(self.labels[idx])"]},{"cell_type":"markdown","metadata":{"id":"4moRG0-VSfGs"},"source":["## 1. Building a VAE"]},{"cell_type":"markdown","metadata":{"id":"VhMioppdSjTp"},"source":["Now that you have your notebook and data set up, complete the following cells to implement your VAE."]},{"cell_type":"markdown","metadata":{"id":"iDRJE-VQ68Uj"},"source":["### 1a: The Encoder/Decoder Model"]},{"cell_type":"markdown","metadata":{"id":"2gqWbtON8gNM"},"source":["First, implement a basic multilayer perceptron model that can be used for the encoder and decoder of your VAE.\n","\n","You can implement any basic MLP, but as a starting point, we recommend one with two linear layers with a ReLU activation function applied between the two layers."]},{"cell_type":"code","execution_count":39,"metadata":{"id":"TWk5YYqB67r7"},"outputs":[],"source":["class MLP(nn.Module):\n","    def __init__(self,\n","        in_size,\n","        out_size,\n","        hidden_size,\n","        act=nn.ReLU,\n","        device='cpu'):\n","        \"\"\"\n","        Multi-layer Perceptron.\n","\n","        Args:\n","            in_size (positive integer): dimensionality of the MLP inputs\n","            out_size (positive integer): dimensionality of the MLP outputs\n","            hidden_size (list): list of sizes for each hidden layer.\n","                The length of this list corresponds to the number of hidden\n","                layers in the MLP.\n","            act (PyTorch function): activation function.\n","            device (string): 'cpu', 'mps', or 'cuda', indicating the hardware\n","                device on which to place all associated parameters and\n","                variables.\n","\n","        Returns:\n","            None.\n","        \"\"\"\n","        super().__init__()\n","\n","        net = []\n","\n","        prev_dim = torch.tensor(in_size)\n","\n","        for hidden in hidden_size:\n","            layer = nn.Linear(prev_dim, hidden)\n","            net.append(layer)\n","            net.append(act())\n","            prev_dim = hidden\n","        \n","        layer = nn.Linear(prev_dim, out_size)\n","\n","        net.append(layer)\n","        self.net = nn.Sequential(*net)\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","    def predict(self, x):\n","        output = self.forward(x)\n","        return torch.argmax(output, dim = 1)\n"]},{"cell_type":"markdown","metadata":{"id":"MXLha4DgsEGE"},"source":["### 1b: Implementing a VAE class"]},{"cell_type":"markdown","metadata":{"id":"lSFI6WxIAueX"},"source":["Here, we provide the basic scaffolding for a VAE model. To complete the implementation, fill in each section with ```\"\"\"YOUR CODE HERE\"\"\"```."]},{"cell_type":"code","execution_count":40,"metadata":{"id":"SbWxLTxUsEGE"},"outputs":[],"source":["class VAE(nn.Module):\n","    def __init__(self, in_size,\n","        enc_size=[32,],\n","        hidden_size=4,\n","        dec_size=[32,],\n","        act=nn.ReLU,\n","        kl_beta=1.0,\n","        prior_mu=0.0,\n","        prior_sigma=1.0,\n","        device='cpu'):\n","        \"\"\"\n","        Variational Autoencoder.\n","\n","        Args:\n","            in_size (positive integer): dimensionality of the input data. For an\n","                autoencoder, this also corresponds to the dimensionality of the\n","                outputs.\n","            enc_size (list): list of sizes for each hidden layer of the encoder\n","                MLP.\n","\n","            hidden_size (positive integer): dimensionlity of the VAE's latent\n","                variables.\n","\n","            dec_size (list): list of sizes for each hidden layer of the decoder\n","                MLP.\n","            act (Pytorch function): activation function.\n","            device (string): 'cpu', 'mps', or 'cuda', indicating the hardware\n","                device on which to place all associated parameters and\n","                variables.\n","\n","        Returns:\n","            None.\n","        \"\"\"\n","        super().__init__()\n","\n","        self.in_size = in_size\n","        self.enc_size = enc_size\n","        self.hidden_size = hidden_size\n","        self.dec_size = dec_size\n","        self.act = act\n","        self.kl_beta = kl_beta\n","        self.device = device\n","\n","        # ----- YOUR CODE HERE ----- #\n","        \n","        self.prior = dist.Normal(loc = prior_mu, scale= prior_sigma)\n","\n","        # ----- YOUR CODE HERE ----- #\n","        self.encoder = MLP(in_size= in_size, out_size= hidden_size * 2, hidden_size= enc_size, act = act, device = device)\n","\n","\n","        # ----- YOUR CODE HERE ----- #\n","        self.decoder = MLP(in_size= in_size, out_size= hidden_size * 2, hidden_size= dec_size, act = act, device = device)\n","\n","    # input dimension to latent dimension- encoder\n","    # decoder = gets out the output size \n","\n","    def encode(self, data_bxd):\n","        \"\"\"\n","        Encodes the input data into the latent space.\n","\n","        Parameters:\n","            data_bxd (torch.Tensor): Input data to be encoded. Should be of shape\n","                (batch_size, num_features).\n","\n","        Returns:\n","            tuple: A tuple (mu_bxd, sigma_bxd) containing the means and standard\n","                deviations of the approximate posterior distribution over the\n","                latent space. Both tensors should be of shape\n","                (batch_size, hidden_size).\n","        \"\"\"\n","        # ----- YOUR CODE HERE ----- #\n","        \n","        params = self.encoder(data_bxd)\n","        dist_size = 2\n","        mu, sigma = torch.split(params, dist_size, dim = 1)\n","        log_sigma = sigma.exp()\n","\n","        return mu, log_sigma\n","    \n","        # return NotImplemented\n","\n","    def decode(self, latents_bxd):\n","        \"\"\"\n","        Decodes the latent variables into the output space.\n","\n","        Parameters:\n","            latents_bxd (torch.Tensor): Latent variables to be decoded. Should be of shape\n","                (batch_size, hidden_size).\n","\n","        Returns:\n","            tuple: A tuple containing the means and standard deviations of the\n","                decoded distribution over the output space. Both tensors should be of shape\n","                (batch_size, num_features).\n","        \"\"\"\n","        # ----- YOUR CODE HERE ----- #\n","\n","        params = self.decoder(latents_bxd)\n","        dist_size = 2\n","        mu, sigma = torch.split(params, dist_size, dim = 1)\n","        log_sigma = sigma.exp()\n","\n","        return mu, log_sigma\n","\n","    def forward(self, data_bxd):\n","        \"\"\"\n","        Performs a forward pass through the VAE model.\n","\n","        Parameters:\n","            data_bxd (torch.Tensor): Input data. Should be of shape (batch_size, num_features).\n","\n","        Returns:\n","            dict: A dictionary containing the output tensors for the mean and\n","                standard deviation of the approximate posterior distribution, mean and\n","                standard deviation of the decoded distribution, log likelihood, KL divergence,\n","                and the total loss.\n","        \"\"\"\n","        # ----- YOUR CODE HERE ----- #\n","\n","        # Generate parameters of the approximate posterior q(z|x) using the encode() function\n","        # Build approximate posterior distribution from parameters (hint: look at torch.distributions)\n","        # Randomly sample from approximate posterior\n","\n","        mu_bxd, logvar_bxd = self.encode(data_bxd)\n","\n","        std_bxd = torch.exp(0.5 * logvar_bxd)\n","\n","        q_z = dist.Independent(dist.Normal(mu_bxd, std_bxd), 1)\n","\n","        z_bxd = q_z.rsample()\n","\n","        # Decode samples\n","        recon_mu_bxd, recon_logvar_bxd = self.decode(z_bxd)\n","\n","        recon_std_bxd = torch.exp(0.5 * recon_logvar_bxd)\n","\n","        log_likelihood = self.log_like(recon_mu_bxd, recon_std_bxd, data_bxd)\n","        \n","        kl_div = self.kl_divergence(q_z)\n","\n","        # Compute loss\n","        total_loss = -torch.mean(log_likelihood - self.kl_beta * kl_div)\n","\n","        return {\n","            'mu': mu_bxd,\n","            'logvar': logvar_bxd,\n","            'recon_mu': recon_mu_bxd,\n","            'recon_logvar': recon_logvar_bxd,\n","            'log_likelihood': log_likelihood,\n","            'kl_divergence': kl_div,\n","            'loss': total_loss\n","        }\n","\n","    def log_like(self, mu_bxd, sigma_bxd, data_bxd):\n","\n","      \"\"\"\n","      Computes the log likelihood of the input data given the latent variables.\n","\n","      Parameters:\n","          mu_bxd (torch.Tensor): Mean of the decoded distribution for each data point. Should be of shape\n","              (batch_size, num_features).\n","          sigma_bxd (torch.Tensor): Standard deviation of the decoded distribution for each data point.\n","              Should be of shape (batch_size, num_features).\n","          data_bxd (torch.Tensor): Input data points. Should be of shape (batch_size, num_features).\n","\n","      Returns:\n","          torch.Tensor: The log likelihood of each data point. Should be of shape (batch_size,1).\n","      \"\"\"\n","\n","      p = dist.Independent(dist.Normal(loc = mu_bxd, scale = sigma_bxd), 1)\n","      log_likelihood = p.log_prob(data_bxd).sum(dim = 1).mean()\n","\n","      return log_likelihood\n","\n","    def kl_divergence(self, approx_posterior):\n","\n","      \"\"\"\n","      Computes the KL divergence between the approximate posterior distribution\n","      (given by 'approx_posterior') and the prior distribution defined during\n","      VAE initialization.\n","\n","      Parameters:\n","          approx_posterior (torch.distributions.Distribution): The approximate\n","              posterior distribution q(z|x), typically obtained from the encoder\n","              of the VAE.\n","\n","      Returns:\n","          torch.Tensor: KL divergence for each sample in the batch.\n","      \"\"\"\n","\n","      prior = self.prior    \n","      kl_div = dist.kl_divergence(approx_posterior, prior).sum(dim = 1).mean()\n","\n","      return kl_div"]},{"cell_type":"markdown","metadata":{"id":"Too4TegHsEGF"},"source":["### 1c: Implement the Main Training Loop for your VAE"]},{"cell_type":"markdown","metadata":{"id":"AZQhEwEg3quM"},"source":["Implement a function which iterates through a dataset and trains the model over a specified number of epochs. Clip the gradient norm to value of 1, like was done for HW4."]},{"cell_type":"code","execution_count":41,"metadata":{"id":"SRDX31_MsEGG"},"outputs":[],"source":["def train(model, dataloader, optimizer,\n","          num_epochs=100,\n","          disp_every=10,\n","          max_norm=1.,\n","          ):\n","    \"\"\"\n","    Trains the Variational Autoencoder (VAE) model.\n","    \n","# Poisson log liklihood \n","\n","    Parameters:\n","        model (nn.Module): The VAE model to be trained.\n","        dataloader (DataLoader): DataLoader object containing the training data.\n","        optimizer (torch.optim.Optimizer): Optimizer to be used for training.\n","        num_epochs (int): Number of training epochs (default is 1).\n","        disp_every (int): Interval for displaying training progress (default is 100).\n","        max_norm (float): Maximum norm value for gradient clipping (default is 1.0).\n","\n","    Returns:\n","        dict: A dictionary containing training metrics including losses, log likelihood (LL),\n","            KL divergence (KL), and gradient norms.\n","            # Per Epoch (should be a list)\n","    \"\"\"\n","    loss_fn = nn.MSELoss()\n","    losses = []\n","    log_likelihood = []\n","    KL = []\n","    grad_norm = []\n","\n","    for epoch in range(num_epochs):\n","        sum_losses = 0\n","        sum_grad_mags = 0\n","        sum_KL = 0\n","        sum_LL = 0 \n","\n","        for batch in enumerate(dataloader):\n","            input, target = batch\n","            optimizer.zero_grad()\n","            output, _ = model(input)\n","            loss = loss_fn(output, target)\n","            sum_losses += loss\n","            loss.backward()\n","\n","            nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n","\n","            sum_grad_mags += np.mean([p.grad.norm() for p in model.parameters()])\n","            optimizer.step()\n","\n","        loss_fn.zero_grad()\n","        avg_loss = (sum_losses / len(dataloader)).item()\n","        losses.append(avg_loss)\n","\n","        avg_grad = (sum_grad_mags / len(dataloader)).item()\n","        grad_norm.append(avg_grad)\n","\n","        if epoch % disp_every == 0:\n","            print('Epoch: ', epoch)\n","            print('Average Loss: ', avg_loss)\n","            print('Average Gradient Norm: ', avg_grad)\n","            # print('Log Likelihood: ', avg_ll)\n","            # print('KL Divergence: ', avg_kl)"]},{"cell_type":"markdown","metadata":{"id":"dU6lVFT9sEGG"},"source":["### 1d: Putting it all together"]},{"cell_type":"markdown","metadata":{"id":"b4EtCz2J3sup"},"source":["First, create an instance of a VAE using your class. Then create an optimizer (use Adam) and pass the parameters of the model as an argument. Finally, create a dataset using the provided dataset class and data. Wrap this dataset in a `DataLoader`, which will handle batching of the data.\n","\n","The specific architecture details of the VAE are up to you. A good starting point would be encoder and decoder sizes (```enc_size```, ```dec_size```) of [32,], a hidden size of 4, batch size of 32, and learning rate of 1e-3. *Note: You should **not** modify ```kl_beta``` = 1.0.*"]},{"cell_type":"code","execution_count":42,"metadata":{"id":"WQv2c3gFsEGG"},"outputs":[],"source":["# ----- YOUR CODE HERE ----- #\n","\n","model = VAE(in_size = 32)\n","\n","optimizer = optim.Adam(model.parameters(), lr = 1e-3)\n","\n","loss_fn = nn.MSELoss()\n","\n","dataset = SimpleDataset(train_data, train_labels)\n","loader = DataLoader(dataset, batch_size = 32)"]},{"cell_type":"markdown","metadata":{"id":"TLEvJKlvsEGG"},"source":["### 1e: Train the VAE"]},{"cell_type":"markdown","metadata":{"id":"9ausU1vf3wIQ"},"source":["Train your model using your training loop. We suggest starting with ```num_epochs```=100."]},{"cell_type":"code","execution_count":43,"metadata":{"id":"Ct7FsAoVsEGH","scrolled":true},"outputs":[{"ename":"TypeError","evalue":"linear(): argument 'input' (position 1) must be Tensor, not int","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[43], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ----- YOUR CODE HERE ----- #\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[41], line 38\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, num_epochs, disp_every, max_norm)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28minput\u001b[39m, target \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     37\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 38\u001b[0m output, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output, target)\n\u001b[1;32m     40\u001b[0m sum_losses \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n","File \u001b[0;32m~/anaconda3/envs/cse599n/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[40], line 124\u001b[0m, in \u001b[0;36mVAE.forward\u001b[0;34m(self, data_bxd)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03mPerforms a forward pass through the VAE model.\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m        and the total loss.\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# ----- YOUR CODE HERE ----- #\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# Generate parameters of the approximate posterior q(z|x) using the encode() function\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# Build approximate posterior distribution from parameters (hint: look at torch.distributions)\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# Randomly sample from approximate posterior\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m mu_bxd, logvar_bxd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_bxd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m std_bxd \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m logvar_bxd)\n\u001b[1;32m    127\u001b[0m q_z \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39mNormal(mu_bxd, std_bxd)\n","Cell \u001b[0;32mIn[40], line 74\u001b[0m, in \u001b[0;36mVAE.encode\u001b[0;34m(self, data_bxd)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03mEncodes the input data into the latent space.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m        (batch_size, hidden_size).\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# ----- YOUR CODE HERE ----- #\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_bxd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m dist_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     76\u001b[0m mu, sigma \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msplit(params, dist_size, dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n","File \u001b[0;32m~/anaconda3/envs/cse599n/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[39], line 43\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/anaconda3/envs/cse599n/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m~/anaconda3/envs/cse599n/lib/python3.8/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m~/anaconda3/envs/cse599n/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m~/anaconda3/envs/cse599n/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not int"]}],"source":["# ----- YOUR CODE HERE ----- #\n","train(model= model, dataloader= loader, optimizer= optimizer)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"eqNV8fKfsEGH"},"source":["### 1f: Generate Plots"]},{"cell_type":"markdown","metadata":{"id":"6N272dP83ycr"},"source":["Plot the loss, log likelihood, KL divergence, and gradient norm throughout training. Create four plots with \"Epoch\" on the x-axis and  \"Loss\", \"LL\", \"KL\", and \"Gradient Norm\" on the y-axes, respectively. For the loss, use a log scale for the y-axis."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"df57-h1isEGH"},"outputs":[],"source":["# ----- YOUR CODE HERE ----- #\n"]},{"cell_type":"markdown","metadata":{"id":"DDgYRmDRh_C6"},"source":["### 1g: Visualizing VAE encodings"]},{"cell_type":"markdown","metadata":{"id":"Ke9hE_QSh_pk"},"source":["Visualize the encoded datapoints. Use the trained VAE's ```encode()``` function to get the means of the approximate posteriors.  If your VAE has a latent dimensionality (```hidden_size```) greater than 2 (we recommended 4 above), apply PCA to further reduce the dimensionality down to 2. Visualize these 2-dimensional embeddings on a scatter plot. Color each dot appropriately according to reaching angle (there should be a total of 728 dots)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uKR6pjU6iBML"},"outputs":[],"source":["# ----- YOUR CODE HERE ----- #\n"]},{"cell_type":"markdown","metadata":{"id":"r7ME3fomYvlj"},"source":["## 2. Building a Simple Classifier"]},{"cell_type":"markdown","metadata":{"id":"oREGKvmz3zv-"},"source":["Now, we would like to build a simple classifier to compare how it performs when trained on the raw data versus when trained on the low-dimensional embeddings encoded by your trained VAE. While it's typical for VAE model encoders to output both the mean and log standard deviation of the approximate posterior distribution, it is common to use the mean only as the encoded features for downstream tasks. We will only use the mean features to train the simple classifier.\n","\n","Define a SimpleClassifier class (PyTorch ```nn.Module```) that is a (relatively) small neural network. One starting point would be two linear layers with a ReLU activation applied between the layers. Then, complete the training loop and evaluation code."]},{"cell_type":"code","execution_count":24,"metadata":{"id":"WgDagz2cVDQg"},"outputs":[],"source":["# ----- YOUR CODE HERE ----- #\n","\n","class SimpleClassifier(nn.Module):\n","    def __init__(self,\n","        in_size,\n","        out_size,\n","        hidden_size,\n","        act=nn.ReLU,\n","        device='cpu'):\n","        \n","        super().__init__()\n","\n","        net = []\n","\n","        prev_dim = in_size\n","        for hidden in hidden_size:\n","            layer = nn.Linear(prev_dim, hidden)\n","            net.append(layer)\n","            net.append(act())\n","            prev_dim = hidden\n","        \n","        layer = nn.Linear(prev_dim, out_size)\n","        net.append(layer)\n","        self.net = nn.Sequential(net)\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","    def predict(self, x):\n","        output = self.forward(x)\n","        return torch.argmax(output, dim = 1)"]},{"cell_type":"markdown","metadata":{"id":"nNdcNOGtSk2y"},"source":["Implement a training loop for the classifier."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Ngcu_WbV5ot"},"outputs":[],"source":["def train_classifier(model, dataloader, criterion, optimizer,\n","          num_epochs=100,\n","          disp_every=10):\n","    \"\"\"\n","    Trains the classifier model.\n","\n","    Parameters:\n","        model (nn.Module): The classifier model to be trained.\n","        dataloader (DataLoader): DataLoader object containing the training data.\n","        criterion (torch.nn.modules.loss._Loss): Loss function to be used.\n","        optimizer (torch.optim.Optimizer): Optimizer to be used for training.\n","        num_epochs (int): Number of training epochs (default is 1).\n","        disp_every (int): Interval for displaying training progress (default is 100).\n","\n","    Returns:\n","        list: A list containing the training losses for each epoch.\n","    \"\"\"\n","    losses = []\n","    # ----- YOUR CODE HERE ----- #\n","\n","\n","    return losses"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NCm_ebRQfPWZ"},"outputs":[],"source":["def evaluate_classifier(model, dataloader):\n","    \"\"\"\n","    Evaluates the classifier model on the test data.\n","\n","    Parameters:\n","        model (nn.Module): The trained classifier model to be evaluated.\n","        dataloader (DataLoader): DataLoader object containing the test data.\n","    \"\"\"\n","    # ----- YOUR CODE HERE ----- #\n"]},{"cell_type":"markdown","metadata":{"id":"P34kbfxt6bvc"},"source":["## 3. Comparing Training Data"]},{"cell_type":"markdown","metadata":{"id":"f5fJAvGe6gNj"},"source":["### 3a: Dataset preparation"]},{"cell_type":"markdown","metadata":{"id":"mnj6GBNk72Fi"},"source":["First, we will need to set up the datasets and DataLoaders for the comparison between the raw data and encoded embeddings. You can reuse the provided ```SimpleDataset``` class for PyTorch Datasets.\n","\n","To get the encoded embeddings, use the ```encode()``` function of your trained VAE. As a reminder, we will be using the mean only as the encoded features."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EIQcy-u_71xA"},"outputs":[],"source":["# Raw data datasets and DataLoaders\n","# ----- YOUR CODE HERE ----- #\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jCHSPytR7bF1"},"outputs":[],"source":["# Encoded data datasets and DataLoaders\n","# ----- YOUR CODE HERE ----- #\n"]},{"cell_type":"markdown","metadata":{"id":"TyhdfhoXZB87"},"source":["### 3b: Train a SimpleClassifier model using the raw data"]},{"cell_type":"markdown","metadata":{"id":"2PwveqyX75Kb"},"source":["Using the defined model from (2) and the datasets/DataLoaders created above, train an instance of the simple classifier using the raw data.\n","\n","We recommend using cross entropy loss and the Adam optimizer, training for 1000 epochs. However, you may change any of the hyperparameters."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oatVJI6vXky3"},"outputs":[],"source":["# ----- YOUR CODE HERE ----- #\n"]},{"cell_type":"markdown","metadata":{"id":"ch6LrcHAdxIp"},"source":["Plot the training loss curve with epochs on the x-axis and loss on the y-axis.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s1J6LE4I_z7o"},"outputs":[],"source":["# ----- YOUR CODE HERE ----- #\n"]},{"cell_type":"markdown","metadata":{"id":"hSmClgBF-FT2"},"source":["### 3c: Train a SimpleClassifier model using the encoded data"]},{"cell_type":"markdown","metadata":{"id":"iqXzr1lH-IfT"},"source":["Uisng the defined model from (2) and the datasets/DataLoaders created above, train an instance of the simple classifier using the encoded data.\n","\n","We recommend using cross entropy loss and the Adam optimizer, training for 100 epochs. However, you may change any of the hyperparameters.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CpWyCW1Jazem"},"outputs":[],"source":["# ----- YOUR CODE HERE ----- #\n"]},{"cell_type":"markdown","metadata":{"id":"VPdSmLiKd4o8"},"source":["Plot the training loss curve with epochs on the x-axis and loss on the y-axis."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_b2g52QvAYYR"},"outputs":[],"source":["# ----- YOUR CODE HERE ----- #\n"]},{"cell_type":"markdown","metadata":{"id":"N4LMXvkg-Srm"},"source":["### 3d: Evaluate and compare classifiers"]},{"cell_type":"markdown","metadata":{"id":"YuRHA49T-XdD"},"source":["Using your defined evaluation code, evaluate the two trained classifiers.\n","\n","1. How many parameters do each of your trained models include?\n","2. Which of the models perform better? Why might this be the case?\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EUGf1BR9ao8o"},"outputs":[],"source":["# ----- YOUR CODE HERE ----- #\n"]},{"cell_type":"markdown","metadata":{"id":"wl1w7jdNiF5F"},"source":["Your answers to the written questions here:\n","\n","1.\n","\n","2."]},{"cell_type":"markdown","metadata":{"id":"xlwVWsQFve_0"},"source":["## 4. Compare with a classifer from HW2"]},{"cell_type":"markdown","metadata":{"id":"fovb7m3tvjPs"},"source":["Now, try applying one of the classifiers you implemented from HW2, Question 4, to the encoded data.\n","\n","1. How does training the classifier on top of the VAE compare to training it on just the raw data?\n","\n","You may choose any of the classifier(s) from HW2, and may change the VAE hyperparameters above. Please list out any important hyperparameters or training details when comparing the classifier trained on top of the VAE to the classifier trained on just the raw data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HoLdLdrlveao"},"outputs":[],"source":["# ----- YOUR CODE HERE ----- #\n"]},{"cell_type":"markdown","metadata":{"id":"qyPmTJD8iO2o"},"source":["Your answers to the written question here:\n","\n","1."]}],"metadata":{"@webio":{"lastCommId":null,"lastKernelId":null},"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.19"}},"nbformat":4,"nbformat_minor":0}
